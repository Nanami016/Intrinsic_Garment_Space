### Learning an Intrinsic Garment Space for Interactive Authoring of Garment Animation

***



This is the demo code for training a motion invariant encoding network. The following diagram provides an overview of the network structure.

For more information, please visit http://geometry.cs.ucl.ac.uk/projects/2019/garment_authoring/ 

<img src=".\resource\network.png" alt="network" width=600 />



The project's directory is shown as follows. The data set is in the `data_set` folder, including cloth mesh(generated by Maya Qualoth), garment template, character animation and skeletons. Some supporting files can be found in `support`. The code is in the `training` folder. The shape feature descriptor and motion invariant encoding network are saved in `nnet`.



```
├─data_set
│  ├─anim
│  ├─case
│  ├─garment
│  └─skeleton
├─nnet
│  ├─basis
│  └─mie
├─support
│  ├─eval_basis
│  ├─eval_mie
│  ├─info_basis
│  └─info_mie
└─training
    └─common
```



In the `training` folder, there are several python scripts which implement the training process. We also provide a data set for testing, generated from a sequence of dancing animation and a skirt.

<img src=".\resource\f1.png" alt="f1" width=300 />

You can run these scripts from 01 to 05, after that, there will be a `*.net` file. It is the shape feature descriptor. If the loss of the descriptor is low enough, you can run 06, 07, 08 scripts to get the motion Invariant encoding network. In addition, 051 and 081 scripts are used for evaluation. If everything goes well, the exported mesh would be like the following figures.

(01 script is used to split `*.npy` file and it is optional)

The result of a specific frame after running 051 script. The yellow skirt is our output and the blue one is the ground truth. 

<img src=".\resource\f2.png" alt="f2" width=300 />



For the output from 081 script is painted by red and the green one is the ground truth.

<img src=".\resource\f3.png" alt="f3" width=300 />